% Hello LaTeX %
\documentclass[10pt,final,journal]{IEEEtran}

\begin{document}
\title{Self-Partitioning Cloud Datastore for Salable Transaction Processing Proposal}
\author{Benjamin Busjaeger, Jonathan Chu, Daniel Ormond \\
\{busjaeg2, jmchu2, ormond1\}@illinois.edu}
\date{Feb 2012}
\maketitle

%\begin{abstract}
%\end{abstract}

%\begin{IEEEkeywords}
%Cloud, OLTP
%\end{IEEEkeywords}

\section{Intorduction}
Cloud Computing has arisen to become one of the fastest growing fields in the industry with companies like Amazon, Rackspace, and Microsoft investing heavily into the cloud. Emerging businesses need to maintain high performance as they scale up to their growing customer base. On the internet, network traffic can be highly volatile and quickly shift from one place to the next. As such websites need to have a distributed database system that can handle a large number of transactions and queries while still guaranteeing consistency in the data. OLTP (Online Transaction Processing) systems that rely on old database technology like SQL struggle to scale when data processing needs to be spread out across many different servers ~\cite{Malkowski:2010:EAD:1774088.1774449}. Therefore, a new class of database technologies called NoSQL have been developed that do not need to follow the requirements of traditional relational database models. These NoSQL databases distribute data across many nodes to scale horizontally and thus ensure fast parallel performance in serving customers.  However, they do not naturally ensure the ACID properties of atomicity, consistency, isolation, and durability that have already been established in the SQL technologies.

Data consistency is very important in online webstore transactions and high performance is crucial for databases to efficiently process customer transactions. Prior research has shown that even a few milliseconds delay can dramatically change customer satisfaction with their web experience. If a website does not finish a transaction in a timely manner, customers may become irritated and take their business elsewhere ~\cite{Ramsay:1998}. A visitor may go through a number of steps while on a website such as clicking a buy button, checking out, processing shipping directions, pushing a credit card transaction, and so on. These steps obviously need to preserve the integrity of the data to avoid unpleasant surprises, such as buying an item which is out of stock. That is why companies should strive to achieve the quickest possible transaction times while still maintaining data consistency.

It is difficult for a programmer to manually partition databases and to see the relationships between data.  He may have to be able to analyze complex data access patterns. Also, manual partitioning puts a burden on the programmer that instead could be handled by software to improve application and development time. By letting the system handle the database partitioning, application developers can spend their time elsewhere in the development of their websites. Finally, there may be certain web applications which have a complex structure and chain of data accesses that may not be feasible for a human to efficiently analyze.

To improve performance, there must be an effective data partitioning scheme to co-locate related data items and allocate them onto nodes to minimize bandwidth within the server cluster and improve response time. We propose to develop a new methodology for partitioning data by utilizing graph theory and machine learning. Graph theory is important in developing a linked topology to determine which sets of data are connected to each other and thus should reside on the same node. Markov chain theory is also useful in determining which steps a user will likely take after an action. Markov chains are useful in determining the likely transitions from one state to the next and have been found to be able to model real world processes very well ~\cite{Gilks:1996}. By leveraging these techniques, and combining them with machine learning algorithms to train on past data access patterns, we believe we can create a new approach for data partitioning that will be more effective than those developed in the past.

\section{Literature Survey}
In response to the lack of scalability and high operational cost of traditional relational DBMSs, several large Internet companies have developed alternative data storage solutions. The most notable systems are Google's BigTable ~\cite{Chang:2006:BDS:1267308.1267323}, Yahoo's PNUTS ~\cite{Cooper:2008:PYH:1454159.1454167}, and Amazon's Dynamo ~\cite{DeCandia:2007:DAH:1323293.1294281}. These key/value databases automate many aspects of distributed storage, but typically restrict ACID transactions to single rows, which is insufficient for many web application use cases. As a result, numerous approaches have been proposed recently  to enhance these key/value stores with transactions that span multiple rows. At the same time several recent research efforts have focused on adapting traditional relational  DMBSs to make them more suitable for cloud deployments. We will first examine these recent OLTP cloud database architectures and subsequently discuss partitioning algorithms related to our work.

\subsection{OLTP Cloud Data Stores}
OLTP cloud databases can be classified according to several salient characteristics. One of them is whether they are built on a key/value store or a relational database. Another is whether or not they employ partitioning at the data model level to reduce or eliminate distributed transactions at runtime. Yet another distinguishing feature is the isolation level supported. Some provide serializability for all operations, others offer serializability but apply weaker isolation for range queries, yet others support only snapshot isolation. In the following we analyze several recent developments with respect to this classification and summarize our findings in Table 1.

Cloud SQL Server ~\cite{Campbell:2010:ESF:1807167.1807280, Bernstein:2011:AMS:2004686.2005651} is the relational DBMS underlying Microsoft's Azure cloud platform. At its core, it is based on Microsoft SQL Server with a design center on scalability through partitioning and primary-copy replication. Serializable ACID transactions are supported, but limited to a single partition to avoid distributed transactions. A partition can be a whole logical database, referred to as a table group, if it is sufficiently small. Alternatevely, a partition can consist of a set of rows from a table group that share a common partitioning key assigned at the schema level by the user.

ElasTraS ~\cite{Das:2009:EET:1855533.1855540, Das:2010:EAE} uses similar ideas to scale-out relational DMBSs to cloud workloads. It uses schema-level partitioning and restricts transactions to single partitions by assigning them to Owning Transaction Managers (OTMs). Mapping between partitions and node is dynamic to provide elasticity. ElasTraS differs from Cloud SQL Server in that it uses a distributed file system to decouple storage from metadata and transaction managment. This design is borrowed from BigTable as are several other aspects of the design.

Relational Cloud ~\cite{Curino:2011:JPMWMBZ11} aims to establish an architecture for provisioning multi-tenant relational databases as a service. It combines a workload-aware approach for efficient data placement with a graph-based partitioning algorithm for co-locating data items frequently accessed together in transactions. ACID transactions are supported within and across partitions as partitions are formed to minimize distributed transactions. The graph-based partitioning algorithm, which is further discussed in the next section,  sets Relational Cloud apart from Cloud SQL Server and ElasTraS.

MegaStore ~\cite{Furman:2008:8530095, Baker:2011:8530095} is Google's highly available data store powering its Web platform service App Engine and many internal services. It extends BigTable with the notion of entity groups, which co-locate rows in the same BigTable partition to provide localized ACID transactions within them. Operations that span multiple entity groups are exposed to weaker consistency semantics. Megastore also builds several other notable features on top of BigTable, which include secondary indexes and synchronous data replication across datacenters.

CloudTPS ~\cite{Zhou:2011:5740834} interposes a group of Local Transaction Managers (LTMs) between clients and key/value data stores to provide full ACID semantics for short transactions. It is designed for web applications and makes several limiting assumptions: transactions are short and access few, aprio known data items. Complex read queries are circumvent the LTMs and thus potentially see an older but consistent snapshot of the data. Scalability is achieved by caching relevant data and transaction state in memory, replicated across LTMs.

G-Store

Deuteronomy ~\cite{Levandoski:2011:8530161} focusses on separating transaction and data management into a transaction component (TC) and data component (DC). All requests go through a single TC. Authors mention that multiple TCs can be used, but they need to serve disjoint data. The authors do not describe how data would be partitioned in this case and the assumption that a single TC can handle all client requests likely does not hold in large cloud deployments (and may have undesirable implications wrt. availability).

HBaseSI ~\cite{} and ReTSO ~\cite{} (

ecStore ~\cite{}

The OLTP data store we plan to build is most closely related to Megastore and builds on several ideas proposed in other solutions.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{OLTP Cloud Data Store Classification}
\label{classification}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\bfseries Data Store  & \bfseries Data Model & \bfseries  Partitioning & \bfseries Isolation \\
\hline
\hline
Cloud SQL Server & relational & yes & serializable \\
ElasTraS & relational & yes & serializable \\
Relational Cloud & relational & yes & serializable \\
Megastore & key/value+ & yes & serializable/snapshot \\
CloudTPS & key/value & no & serializable/snapshot \\
G-Store & key/value & yes (dynamic) & serializable \\
Deuteronomy & ? & no &serializable \\
HBaseSI & key/value & ? & snapshot \\
ReTSO/Omid & key/value & ? & snapshot \\
ecStore & ? & no & ? \\
\hline
\end{tabular}
\end{table}

\input{partitioning-algos}



\section{Methods}

\subsection{Foundation}
To implement our system, we will utilize and build off the tools in the Apache Hadoop project. We will specifically utilize the HDFS project and the HBase project. The Hadoop Distributed File System is a popular open source storage system created to be used for Hadoop applications.  It has a master/slave architecture wihch consists of a central Namenode server that monitors the system and multiple DataNodes which actually contain the data.  The NameNode contains all the metadata and responds to remote procedure calls(RPC) from the DataNodes.  It is highly scalable in growing up to large clusters for applications which will need to process heavy workloads and has been utilized in clusters that contain thousands of nodes.  HDFS communication protocols operate at the internet TCP/IP protocol making it ideal for web applications. Also data integrity is assured as HDFS implements checksum checking on files.  By checking the checksum which is generated after file creation, one can check whether any bits have been misplaced after file transfer by recomputing the checksum on the newly transferred file and comparing it to the old checksum to see if they match correctly.  If they do not then the file will be marked as corrupted and the system may take the necessary steps to correct the error, perhaps by making a new file.  Data replication is also handled by this application to make the application fault tolerant against transient failures in the nodes such as a server crash or a network outage.  HDFS will automatically pair a user with the closest replica of the data nearest to it.  This ensures the quickest possible time for transferring data to a user if an application is supported on multiple datacenters across a wide area network.  Since the internet spans the entire globe, there can be significant improvements in accessing data from a local datacenter as opposed to waiting for data to travel thousands of miles to wait for a response.

HDFS is written in the Java programming language which means that development is easier and less bug prone than other languages.  There is a Java API associated with HDFS making it easy to interact with for application developers.  Also, Java has a large support base and is popularly known in the software world.  This means that other developers can easily learn how to use HDFS without having to go through a long learning curve typically associated with distributed systems built using more niche languages like Erlang.  Also, Java has a great deal of support and can run Java bytecode on any machine that has the Java Virtual Machine installed.

On top of HDFS, we will run the HBase database which is one of the most popular NoSQL databases. Traditional RDBMS do not scale well to many machines and thus create a bottleneck for companies that have a lot of network traffic to process. The distribution of traffic and application processing across multiple nodes is the primary way to scale for large companies as opposed to trying to handle all the traffic from one server.  HBase is a structured column-oriented store that is an open source clone of Google’s BigTable.  HBase introduces the structure of tables and allows these tables to be automatically sharded and configured across multiple nodes.  Thus HBase is linearly scalable to large cluster sizes.  HBase allows for files to be quickly looked up on HDFS by indexing files.  It hierarchically arranges objects into Table -> Region -> Store -> MemStore/StoreFile -> Block.  It seperates a cluster into regions with each region being controlled by a RegionServer.  The HMaster is the Master Server which monitors all the RegionServers and each region may consist of multiple nodes.

Like HDFS, HBase also has a Java API to allow application developers to interface with it.  It also supports a variety of other APIs like Thrift and Rest for non-Java developers.

\subsection{Middleware}
For our project we will implement a middleware that handles the transactions and stores the transactions before being loaded onto HBase for persistence.  This middleware will consist of multiple nodes that can handle different transactions in parallel.  It is this middleware that we will be handling our data partitioning.  To access data efficiently, the data will be stored in main memory so that disk reads do not have to occur.  Reading from random access memory is much faster than going to the disk space so ideally the data that a customer needs to access will be already on main memory.  To achieve this, we need to devise a partitioning scheme that is highly optimized in placing data where transactions can access it quickly.  Also, transactions should not have to jump from one node to the next retrieving data.  That type of data transfer within a server cluster would waste bandwidth.  Therefore our algorithm should be able to analyze the data access patterns and arrange data accordingly.

Graph theory has led to a variety of useful algorithms and concepts that should come in handy for the database partitioning problem.  Most graph partitioning problems are NP-hard so there are heuristic solutions available which can generate the appropriate node topology.  For example the Kernighan-Lin algorithm partitions two disjoint subsets so that the weight of the edges between them is minimized.  This is a technique which has traditionally been used in digital circuits, but may have uses in database partitioning.  Hypergraph theory is another promising idea in which one edge can connect to multiple vertices.  This type of a structure may be useful in simulating a distributed database topology where one network link may connect to a region with multiple nodes.  The Max-flow min-cut theorem is also useful in determining the weak cuts in a network that could cause the system to crash.  By using this theorem one could create more fault tolerant data centers in which there are few weakpoints that could stop important traffic in a system.

Clustering is another popular technique used in the field of machine learning to arrange similar data items together.  A specific form of clustering called hierarchical clustering involves minimizing the distance between related data items.  This would be useful in the database setting to minimize the network delay that a packet of data may experience traveling from one node to the next.  It would be especially useful in datacenters that span a wide area network where distance is indeed a major problem.  Centroid-based clustering is also useful because it finds the point within a cluster that has the shortest total distance to the other nodes.  This can be useful in determining where to place the central master node which has to communicate with all the nodes in its region.  A more general extension to this theory would involve Voronoi diagrams which arranges a diagram into multiple cells such that points in a cell are closer to each other than they are to points in other cells.

Markov chains are random processes which transition from one state to the next without retaining memory.  If we model internet behavior as stochastic processes perhaps we may be better able to predict which data item a user may request to access next.  In this way, we may be able to improve the performance of databases by prefetching data items before they are requested.  This way, the user would not have to wait for data to travel from one server node to the next.  Google uses Markov chains for its PageRank algorithm to predict which websites a user will be most interested in visiting next given their current location[3].  Markov chains have also been used in queueing theory for networks to model the flow of packets coming in[4].

\subsection{Architecture}
To implement the middleware we will be programming in JAVA so that our program can be developed quickly without worrying too much about bugs.  The Middleware layer will consist of the OLTP servers which will handle the transaction and query requests from the application layer.  We will pipeline data from the OLTP servers to a data warehouse.  The purpose of the data warehouse will be to create a modular abstraction that can repartition the OLTP server as a separate entity.  The data warehouse will have a structure that is optimized to run the complex graph algorithms, clustering, and Markov chaining.  This will improve the efficiency of the learning and partitioning phase since the OLTP servers will most likely not have the data in a form which will be able to be naturally parsed for the purposes of analysis.

The data warehouse will be a relational database that will be an optimized OLAP center for the system.  It should made as modular as possible so that it can be attached or detached to existing database implementations.  By creating this data warehouse abstraction, users will not have to modify existing OLTP implementations and can let the data warehouse take care of the data partitioning as a third entity.  This will create a system that is portable and easy to use.

The middleware will stand between the application layer and the HBase layer.  In this way it can optimize data retrieval for the application layer while still persisting data to the HBase layer in case of faults occurring within the middleware.  This will make our system reliable and yet highly efficient

In order to effectively train, relevant metadata associated with each data item must be maintained such as access time, access patterns, size of data, and etc.  This metadata must travel to the OLAP center to be used for data analysis.  Moreover the topology of the network should be shown to the OLAP center like the number of nodes in the system, network connections, bandwidth, size of memory, size of hard disk, cpu speed, location and etc.  The information needed within a query request should be able to be obtained with as few round trips as possible, so there should be a mechanism to efficiently process requests without having to go through a see saw motion of requesting and replying.

\section{Results}
We expect to implement a middleware on top of Hbase that will have additional performance benefits in the area of OLTP.  Our middleware should be able to be highly efficient while still maintaining consistency guarantees that are needed for transactions.  First we will learn how to implement an HDFS and HBase distribution.  Then we will program a middleware in JAVA to interact with HBase and implement our partitioning algorithms.  We will then try to test our program on one of the public workload benchmarks to compare our performance with other designs.

We expect the automatic partitioning to introduce some overhead into the system. However, we hope to design the algorithm such that the overhead is minimal for relatively static access patterns so that the cost is amortized. We will also attempt to keep the overhead of partitioning the data small.

\section{Discussion}


% Bibliography generation from references.bib
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
