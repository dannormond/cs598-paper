% Hello LaTeX %
\documentclass[10pt,final,journal]{IEEEtran}

\begin{document}
\title{Self-Partitioning Cloud Datastore for Salable Transaction Processing Proposal}
\author{Benjamin Busjaeger, Jonathan Chu, Daniel Ormond \\
\{busjaeg2, jmchu2, ormond1\}@illinois.edu}
\date{Feb 2012}
\maketitle

%\begin{abstract}
We are designing a new cloud computing system that will be highly scalable and yet still be able to maintain the ACID properties of relational database management systems. Our system forms a middle tier between the application and datastore layer to create modularity in the architecture and ease integration. We plan to utilize graph theory, clustering algorithms, and markov chaining to partition the data within our system into optimal locations in the server clusters. By training on historical logs, our system should be able to recognize data access patterns and use that knowledge to partition the data within the cluster to better fit within those patterns. With our data partitioning scheme, we hope to lower time delays, reduce bandwidth, and ultimately improve the speed of online transaction processing while not sacrificing data consistency.
%\end{abstract}

%\begin{IEEEkeywords}
Cloud, datastore, OLTP, Partitioning
%\end{IEEEkeywords}


\section{Intorduction}
Cloud Computing has arisen to become one of the fastest growing fields in the industry with companies like Amazon, Rackspace, and Microsoft investing heavily into the cloud. Emerging businesses need to maintain high performance as they scale up to their growing customer base. On the internet, network traffic can be highly volatile and quickly shift from one place to the next. As such websites need to have a distributed database system that can handle a large number of transactions and queries while still guaranteeing consistency of the data. OLTP (Online Transaction Processing) systems that rely on traditional database technology like SQL struggle to scale when data processing needs to be spread out across many different servers ~\cite{Malkowski:2010:EAD:1774088.1774449}. Therefore, a new class of database technologies called NoSQL have been developed that do not need to follow the requirements of traditional relational database models. These NoSQL databases distribute data across many nodes to scale horizontally and thus ensure fast parallel performance in serving customers.  However, they do not naturally ensure the ACID properties of atomicity, consistency, isolation, and durability that have already been established in the SQL technologies.

Data consistency is very important in online webstore transactions and high performance is crucial for databases to efficiently process customer transactions. Prior research has shown that even a few milliseconds delay can dramatically change customer satisfaction with their web experience. If a website does not finish a transaction in a timely manner, customers may become irritated and take their business elsewhere ~\cite{Ramsay:1998}. A visitor may go through a number of steps while on a website such as clicking a buy button, checking out, processing shipping directions, pushing a credit card transaction, and so on. These steps obviously need to preserve the integrity of the data to avoid unpleasant surprises, such as buying an item which is out of stock. That is why companies should strive to achieve the quickest possible transaction times while still maintaining data consistency.

It is difficult for a programmer to manually partition databases and to see the relationships between data. He may have to be able to analyze complex data access patterns. Also, manual partitioning puts a burden on the programmer that instead could be handled by software to improve application and development time. By letting the system handle the database partitioning, application developers can spend their time elsewhere in the development of their websites. Finally, there may be certain web applications which have a complex structure and chain of data accesses that may not be feasible for a human to efficiently analyze.

To provide scalable performance, an effective data partitioning scheme is needed which co-locates related data and allocates partitions to nodes such that bandwidth consumption within the server cluster is minimized and response time reduced. We propose to develop a  transactional datastore using a new methodology for partitioning data that utilizes graph theory and machine learning. Graph theory is important in developing a linked topology to determine which sets of data are connected to each other and thus should reside on the same node. Markov chain theory is also useful in determining which steps a user will likely take after an action. Markov chains help determine the likely transitions from one state to the next and have been found to be able to model real world processes very well ~\cite{Gilks:1996}. By leveraging these techniques, and combining them with machine learning algorithms to train on past data access patterns, we believe we can create a new approach for data partitioning that will be more effective than those developed in the past.

\section{Literature Survey}
The most notable NoSQL cloud datastores have been developed by large Internet companies in response to the lack of scalability and high operational cost of traditional relational DBMSs. They include Google's BigTable ~\cite{Chang:2006:BDS:1267308.1267323}, Yahoo's PNUTS ~\cite{Cooper:2008:PYH:1454159.1454167}, and Amazon's Dynamo ~\cite{DeCandia:2007:DAH:1323293.1294281}. These key/value databases automate many aspects of distributed storage, but provide limited consistency guarantees and lack partitioning schemes optimzed for transaction processing. As a result, numerous approaches have been proposed recently  to extend these key/value stores with transactions that span multiple rows and tables. At the same time several designes have been developed that adapt traditional relational DMBSs to make them more suitable for cloud worloads. We will first examine these OLTP cloud datastore architectures and subsequently survey relevant work on optimized data partitioning and placement.

\subsection{OLTP Cloud Data Stores}
OLTP cloud datastores can be classified according to several salient characteristics. One of them, as previously mentioned, is whether they are based on key/value datastores or relational databases. Another is whether or not they employ partitioning at the data model level to reduce or eliminate distributed transactions at runtime. Yet another distinguishing feature is the supported isolation level. Some provide serializability for all operations, others offer serializability but only support weaker isolation for range queries, yet others support snapshot isolation. In the following we analyze several recent developments with respect to this classification and summarize our findings in Table 1.

Cloud SQL Server ~\cite{Campbell:2010:ESF:1807167.1807280, Bernstein:2011:AMS:2004686.2005651} is the relational DBMS underlying Microsoft's Azure cloud platform. At its core, it is based on Microsoft SQL Server with a design center on scalability through partitioning and primary-copy replication. Serializable ACID transactions are supported, but limited to a single partition to avoid distributed transactions. A partition can be a whole logical database, referred to as a table group, if it is sufficiently small, or a set of rows from a table group that have been assigned a common partitioning key by the user in the database schema.

ElasTraS ~\cite{Das:2009:EET:1855533.1855540, Das:2010:EAE} uses similar ideas to scale-out relational DMBSs to cloud workloads. It uses schema-level partitioning and restricts transactions to single partitions by assigning them to Owning Transaction Managers (OTMs). Mapping between partitions and node is dynamic to provide elasticity. ElasTraS differs from Cloud SQL Server in that it uses a distributed file system to decouple storage from metadata and transaction managment. This design is borrowed from BigTable as are several other aspects of the design.

Relational Cloud ~\cite{Curino:2011:JPMWMBZ11} aims to establish an architecture for provisioning multi-tenant relational databases as a service. It combines a workload-aware approach for efficient data placement with a graph-based partitioning algorithm for co-locating data items frequently accessed together in transactions. ACID transactions are supported within and across partitions. The graph-based partitioning algorithm, which is further discussed in the next section,  sets Relational Cloud apart from Cloud SQL Server and ElasTraS, which use tree-based partitioning based on user-defined keys.

MegaStore ~\cite{Furman:2008:8530095, Baker:2011:8530095} is Google's highly available data store powering its Web platform service App Engine and many internal services. It extends BigTable with the notion of entity groups, which co-locate rows in the same BigTable partition to provide localized ACID transactions. Operations that span multiple entity groups are exposed to weaker consistency semantics. Megastore also builds several other notable features on top of BigTable, which include secondary indexes and synchronous data replication across datacenters.

G-Store ~\cite{Das:2010:GSD:1807128.1807157} 

CloudTPS ~\cite{Wei:2011:5740834} interposes a group of Local Transaction Managers (LTMs) between clients and key/value data stores to provide full ACID semantics. It is designed for web application workloads and makes the limiting assumptions that transactions are short-lived, access few data items known beforehand, and can return older but consistent snapshots of data for complex read queries. The last assumption has been removed by more recent work ~\cite{Wei:2011:CJQ}, which adds consistent join query support. Data items accessed by transactions are loaded into memory of LTMs using consistent hashing and eventually written back to the datastore. Transaction state is also maintained in memory. Atomicity is achieved by two phase commit across LTMs and durability is ensured by replicating in-memory state across LTMs.

Deuteronomy ~\cite{Levandoski:2011:8530161} focusses on separating transaction and data management into a transaction component (TC) and a data component (DC). The TC only expects a record-oriented interface with atomic operations from the DC, but is otherwise agnostic of how data is stored. The TC provides full ACID guarantees through concurrency control and undo/redo logging that operates on logical as opposed to physical entities. This makes it portable across a variety of data storage solutions. The system relies on a centralized TC to handle all requests, which may not be feasible for large cloud deployments.

HBaseSI ~\cite{} and ReTSO ~\cite{}

ecStore ~\cite{}

The design we envision synthesizes ideas from several of these approaches with our unique paritioning algorithm to provide a strongly consistent key/value datastore.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{OLTP Cloud Data Store Classification}
\label{classification}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\bfseries Data Store  & \bfseries Data Model & \bfseries  Partitioning & \bfseries Isolation \\
\hline
\hline
Cloud SQL Server & relational & yes & serializable \\
ElasTraS & relational & yes & serializable \\
Relational Cloud & relational & yes & serializable \\
Megastore & key/value & yes & serializable/snapshot \\
G-Store & key/value & yes (logical) & serializable \\
CloudTPS & key/value & no & serializable/snapshot \\
Deuteronomy & agnostic & no &serializable \\
HBaseSI & key/value & no & snapshot \\
ReTSO/Omid & key/value & no & snapshot \\
ecStore & ? & no & ? \\
\hline
\end{tabular}
\end{table}

\input{partitioning-algos}



\section{Methods}

\subsection{Foundation}
To implement our system, we will utilize and build off the tools in the Apache Hadoop project. Specifically, we will utilize the Hadoop Distributed File System (HDFS) project and the Hadoop database (HBase) project. HDFS is a popular open source implementation of the Google File System (GFS) ~\cite{Ghemawat:2003:GFS:1165389.945450}. It has a master/slave architecture wihch consists of a central Namenode server that monitors the system and multiple DataNodes which actually contain the data. The NameNode contains all the metadata and responds to remote procedure calls(RPC) from clients and DataNodes. It is highly scalable in growing up to large clusters for applications which will need to process heavy workloads and has been utilized in clusters that contain thousands of nodes.  HDFS communication protocols operate at the internet TCP/IP protocol making it ideal for web applications. Also, data integrity is assured as HDFS implements checksum verification on files. By computing the checksum on a transferred file and comparing it to the stored checksum, HDFS can ensure that no bits have been lost or misplaced during file transfer. If an inconsistency is detected, the file is marked as corrupted and the system may take the necessary steps to correct the error.  Data replication is also handled transparently, which makes HDFS fault tolerant with respect to transient failures in the nodes such as a server crash or a network outage. HDFS will automatically pair clients with the replica of the data nearest to it. This ensures the shortest possible transfer time to a user if an application is deployed across multiple datacenters on a wide area network. Since the Internet spans the entire globe, there can be significant improvements in accessing data from a local datacenter as opposed to waiting for data to travel thousands of miles to wait for a response. Files in HDFS are typically immutable, which simplifies many tasks such as replication, but also provide an append operation, which is particularly useful for writing transaction logs. Finally, it should be noted that HDFS is written in the Java programming language and also exposes Java APIs, which has several advantages, such as ease of development, broad community support, and portability. The inherent scalability, data replication, and consistency features provide a powerful abstraction that facilitates the task of building a distributed database.

On top of HDFS, we will run the HBase database which is one of the most popular NoSQL databases. It is an open source implementation of Google's BigTable ~\cite{Chang:2006:BDS:1267308.1267323} for storing structured data in a versioned, column-oriented table format. Tables are automatically sorted by row key, partitioned into row ranges (regions), and distributed across so called region servers. Region servers store regions in HDFS files in the form of indexed blocks to make them quickly accessible. Since HDFS files are immutable, any data updates are stored in memory and appended to a write-ahead log file using HDFS's append capability. Region servers run several compaction algorithms to periodically flush in-memory data by merging it with existing data files and writing out consolidated data files. Old data files are removed in the process. Clients connect to region servers via a master server, which maintains the mapping from row ranges to region servers. They subsequently cache the information and only need to retrieve it again when the accessed row range is assigned to a different region server, which happens infrequently for a given row range. To increase the storage and processing capacity new region servers can be added to an existing cluster. This design makes HBase linearly scalable to large cluster sizes. HBase provides full ACID transactions for single row operations; modifications to multiple rows can be submitted in batch, but are not guaranteed to complete atomically. Like HDFS, HBase also has a Java API to allow application developers to interface with it.  It also supports a variety of other APIs like Thrift and Rest for non-Java developers.

The final foundation component we expect to utilize is Apache Zookeeper, which is also used by HBase for several coordination tasks, such as monitoring region servers. Zookeeper is an open source project that implements a modified version of the Paxos algorithm ~\cite{}.



\subsection{Middleware}
For our project we will implement a middleware that handles the transactions and stores the transactions before being loaded onto HBase for persistence.  This middleware will consist of multiple nodes that can handle different transactions in parallel.  It is this middleware that we will be handling our data partitioning.  To access data efficiently, the data will be stored in main memory so that disk reads do not have to occur.  Reading from random access memory is much faster than going to the disk space so ideally the data that a customer needs to access will be already on main memory.  To achieve this, we need to devise a partitioning scheme that is highly optimized in placing data where transactions can access it quickly.  Also, transactions should not have to jump from one node to the next retrieving data.  That type of data transfer within a server cluster would waste bandwidth.  Therefore our algorithm should be able to analyze the data access patterns and arrange data accordingly.

Graph theory has led to a variety of useful algorithms and concepts that should come in handy for the database partitioning problem.  Most graph partitioning problems are NP-hard so there are heuristic solutions available which can generate the appropriate node topology.  For example the Kernighan-Lin algorithm partitions two disjoint subsets so that the weight of the edges between them is minimized.  This is a technique which has traditionally been used in digital circuits, but may have uses in database partitioning.  Hypergraph theory is another promising idea in which one edge can connect to multiple vertices.  This type of a structure may be useful in simulating a distributed database topology where one network link may connect to a region with multiple nodes.  The Max-flow min-cut theorem is also useful in determining the weak cuts in a network that could cause the system to crash.  By using this theorem one could create more fault tolerant data centers in which there are few weakpoints that could stop important traffic in a system.

Clustering is another popular technique used in the field of machine learning to arrange similar data items together.  A specific form of clustering called hierarchical clustering involves minimizing the distance between related data items.  This would be useful in the database setting to minimize the network delay that a packet of data may experience traveling from one node to the next.  It would be especially useful in datacenters that span a wide area network where distance is indeed a major problem.  Centroid-based clustering is also useful because it finds the point within a cluster that has the shortest total distance to the other nodes.  This can be useful in determining where to place the central master node which has to communicate with all the nodes in its region.  A more general extension to this theory would involve Voronoi diagrams which arranges a diagram into multiple cells such that points in a cell are closer to each other than they are to points in other cells.

Markov chains are random processes which transition from one state to the next without retaining memory.  If we model internet behavior as stochastic processes perhaps we may be better able to predict which data item a user may request to access next.  In this way, we may be able to improve the performance of databases by prefetching data items before they are requested.  This way, the user would not have to wait for data to travel from one server node to the next.  Google uses Markov chains for its PageRank algorithm to predict which websites a user will be most interested in visiting next given their current location[3].  Markov chains have also been used in queueing theory for networks to model the flow of packets coming in[4].

\subsection{Architecture}
To implement the middleware we will be programming in JAVA so that our program can be developed quickly without worrying too much about bugs.  The Middleware layer will consist of the OLTP servers which will handle the transaction and query requests from the application layer.  We will pipeline data from the OLTP servers to a data warehouse.  The purpose of the data warehouse will be to create a modular abstraction that can repartition the OLTP server as a separate entity.  The data warehouse will have a structure that is optimized to run the complex graph algorithms, clustering, and Markov chaining.  This will improve the efficiency of the learning and partitioning phase since the OLTP servers will most likely not have the data in a form which will be able to be naturally parsed for the purposes of analysis.

The data warehouse will be a relational database that will be an optimized OLAP center for the system.  It should made as modular as possible so that it can be attached or detached to existing database implementations.  By creating this data warehouse abstraction, users will not have to modify existing OLTP implementations and can let the data warehouse take care of the data partitioning as a third entity.  This will create a system that is portable and easy to use.

The middleware will stand between the application layer and the HBase layer.  In this way it can optimize data retrieval for the application layer while still persisting data to the HBase layer in case of faults occurring within the middleware.  This will make our system reliable and yet highly efficient

In order to effectively train, relevant metadata associated with each data item must be maintained such as access time, access patterns, size of data, and etc.  This metadata must travel to the OLAP center to be used for data analysis.  Moreover the topology of the network should be shown to the OLAP center like the number of nodes in the system, network connections, bandwidth, size of memory, size of hard disk, cpu speed, location and etc.  The information needed within a query request should be able to be obtained with as few round trips as possible, so there should be a mechanism to efficiently process requests without having to go through a see saw motion of requesting and replying.

\section{Results}
We expect to implement a middleware on top of Hbase that will have additional performance benefits in the area of OLTP.  Our middleware should be able to be highly efficient while still maintaining consistency guarantees that are needed for transactions.  First we will learn how to implement an HDFS and HBase distribution.  Then we will program a middleware in JAVA to interact with HBase and implement our partitioning algorithms.  We will then try to test our program on one of the public workload benchmarks to compare our performance with other designs.

We expect the automatic partitioning to introduce some overhead into the system. However, we hope to design the algorithm such that the overhead is minimal for relatively static access patterns so that the cost is amortized. We will also attempt to keep the overhead of partitioning the data small.

\section{Discussion}


% Bibliography generation from references.bib
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
