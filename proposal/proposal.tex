% Hello LaTeX %
\documentclass[10pt,final,journal]{IEEEtran}

\begin{document}
\title{Self-Partitioning Cloud Data Store for Salable Transaction Processing Proposal}
\author{Benjamin Busjaeger, Jonathan Chu, Daniel Ormond \\
\{busjaeg2, jmchu2, ormond1\}@illinois.edu}
\date{Feb 2012}
\maketitle

%\begin{abstract}
%\end{abstract}

%\begin{IEEEkeywords}
%foo, bar, x, y, z
%\end{IEEEkeywords}

\section{Intorduction}
Cloud Data Stores need to fulfill several requirements to deliver on the desired economic benefits [cloudonomics]. They need to efficiently pool resources to support large volumes of data or multiple smaller databases, dynamically and elastically scale as data or the number of hosted databases grows or shrinks, be highly available (i.e. fault-tolerant) given the use of commodity hardware, and do all this with minimal human intervention to minimize cost and downtime. These requirements have been referred to as cloud features of data stores.

Traditional relational databases either do not meet these requirements or are too expensive to operate at the required scale [DB benchmark]. At a certain cluster size they inevitable hit the challenges of Brewer's CAP conjecture [CloudSQL quote]. Therefore, in recent years a number of alternatives have been implemented by large companies and documented for the research community. These Key-Value stores trade off expressiveness and consistency for high scalability and availability. The most prominent examples are Google's BigTable ~\cite{Chang:2006:BDS:1267308.1267323}, Yahoo's PNUTS ~\cite{Cooper:2008:PYH:1454159.1454167}, and Amazon's Dynamo ~\cite{DeCandia:2007:DAH:1323293.1294281}.

These Key-Value data stores typically restrict ACID transactions to operations accessing a single row.  This allows for highly flexible implementations and is sufficient for many applications given the denormalized data model ~\cite{Agrawal:2010}. However, many applications have stricter consistency guarantees [...]. In these cases the burden of ensuring consistency across entities is placed on application developers, which is not always feasible or desirable ~\cite{Bernstein:2011:AMS:2004686.2005651}. Therefore, various approaches have been proposed recently  to extend transaction support for Cloud data stores to span multiple rows without compromising too much on their inherent scalability and availablitliy ~\cite{Baker:2011:8530095, Das:2010:GSD:1807128.1807157, Junqueira:2011:LTS:2056318.2057148, Levandoski:2011:8530161, Zhou:2011:5740834, Peng:2010:LIP:1924943.1924961, Zhang:2010:5697970}. At the same time novel approaches for scaling traditional relational DBMSs to Cloud deployments have been proposed ~\cite{Bernstein:2011:AMS:2004686.2005651, Curino:2011:JPMWMBZ11}. 

All of these approaches are based on the premise that distributed transactions do not scale for disk-based shared-nothing data storage architectures [Curino:2010:SWA:1920841.1920853]. Many of the proposals rely on some notion of static or dynamic data grouping to limit the scope within which strong consistency guarantees are provided. For example, Megastore ~\cite{Baker:2011:8530095} exposes statically defined entity groups, CloudTPS ~\cite{Zhou:2011:5740834} dynamically establishes groups across transaction managers, and G-Store ﻿~\cite{Das:2010:GSD:1807128.1807157} defines a protocol for creating and destroying key groups at runtime before and after executing transactions across data items. This approach bridges the need for strong consistency and performance, specifically for applications that exhibit access locality. However, it places the burden of identifying and defining groups on application developers, which makes these solutions more complicated and error prone than those offered in traditional centralized relational DBMSs.

We propose an approach to dynamically detect and establish groups at runtime by trying to anticipate which data items will be accessed together by transactions. We plan to do this by monitoring the transaction log and applying clustering, graph, or learning algorithms. The observed groupings can then be used to physically co-locate data to reduce the number of distributed transactions needed. This repartitioning has to be done live without interrupting service. The basic idea underlying our proposal was motivated in ~\cite{Das:2011} for G-Store ﻿~\cite{Das:2010:GSD:1807128.1807157}.


, which is designed specifically for groups that are transient in nature, but exist long enough to amortize the cost of group formation (e.g. online games in which users join and leave groups).  Finally, Albatross [DNA11] defines an algorithm for live repartitioning of data, which we may be able to apply.

We plan to evaluate our algorithm by implementing it as an extension to one or more of the existing approaches or to a new prototype. The former is preferable, however, not all research prototypes have published source code. One possibility is to extend CloudTPS, since source code is available and the in-memory transaction processing design may lend itself well to runtime partitioning.

\section{Literature Survey}

\subsection{OLTP Cloud Data Stores}
There are several salient features of OLTP Cloud Data Stores that can be used to classify them. whether they partition data to reduce distributed transactions or not. Second, whether they extend key/value stores with stronger consistency guarantees or whether modify existing relational databases to make them meet Cloud requirements. Third, whether they provide serializability or a waker form of isolation such as snapshot isolation. Fourth, ... We use the first classification scheme in the following description of the different solutions, which is consistent with [DAS11]. At the end of this subsection we provide a table which summarizes how the various systems fit into the classification schemes enumerated above.

\subsubsection{Partitioning based Approaches}
Cloud SQL Server ~\cite{Campbell:2010:ESF:1807167.1807280, Bernstein:2011:AMS:2004686.2005651} aims to achieve fast and predictable performance for a relational DBMS by limiting transactions to single nodes, i.e. avoiding distributed transactions. To ensure this, it exposes partitioning keys in the schema and allows transactions to only access data items with the same partitioning key (in the same logical DB). Apart from the performance concern, distributed transactions are avoided as participants can get blocked if the coordinator fails during the uncretainty period; to unblock, system needs to guess or needs operator intervention.

MegaStore ~\cite{Furman:2008:8530095, Baker:2011:8530095}

ElasTraS ~\cite{Das:2009:EET:1855533.1855540, Das:2010:EAE} proposes a design similar to that of BigTable to partition a relational database and provide strong consistency guarantees within partitions by assinging each partition to an owning transaction manager. Mapping between partition and nodes is dynamic and elastic (differs from traditional DBMS). Although ElasTraS does not require a specific partitioning scheme, it proposes schema level partitioning, specifically a hierarchical scheme similar to Megastore's entity groups. In this scheme partitioning done by root table key.

G-Store ~\cite{Das:2010:GSD:1807128.1807157}

Relational Cloud ~\cite{Curino:2011:JPMWMBZ11}

\subsubsection{Non-Partitioning based Approaches}
CloudTPS ~\cite{Zhou:2011:5740834} designed for common web application access patterns: short transactions, few data items, and many read-only for which older yet consistent data acceptable. Interposes a group of so called Local Transaction Managers between clients and data store that cache relevant data and maintain transaction state in memory. Durability is achieved by replicating transaction state across LTMs, atomicity via 2PC across LTMs, and isolation via group leadership protocol.

Deuteronomy ~\cite{Levandoski:2011:8530161} focusses on separating the concerns of transaction and data management into a transaction component (TC) and data component (DC). All requests go through a single TC. Authors mention that multiple TCs can be used, but they need to serve disjoint data. The authors do not describe how data would be partitioned in this case and the assumption that a single TC can handle all client requests likely does not hold in large cloud deployments (and may have undesirable implications wrt. availability).

Percolator ~\cite{Peng:2010:LIP:1924943.1924961}

ecStore ~\cite{}

Scalaris 

HBaseSI

ReTSO (Omid)

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{OLTP Cloud Data Store Classification}
\label{classification}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\bfseries Data Store  & \bfseries Data Model & \bfseries  Partitioning & \bfseries Isolation \\
\hline
\hline
Cloud SQL Server & relational & yes & serializable \\
CloudTPS & key/value & no & serializable \\
Deuteronomy & ? & no &serializable \\
ecStore & ? & no & ? \\
ElasTraS & relational & yes (static) & serializable \\
G-Store & key/value & yes (dynamic) & serializable \\
HBaseSI & key/value & ? & snapshot \\
H-Store & ? & ? & ? \\
Megastore & key/value+ & yes (static) & serializable \\
Percolator & key/value & no & ? \\
RelationalCld & relational & yes (statis) & serializable \\
ReTSO/Omid & key/value & ? & snapshot \\
Sinfonia & ? & ? & ? \\
Scalaris & ? & ? & ? \\
\hline
\end{tabular}
\end{table}

\subsection{Paritioning Algorithms}
Schism ~\cite{Curino:2010:SWA:1920841.1920853} is a static partitioning algorithm to reduce distributed transactions for SQL datastores. It significant improves performance compared to hash-based and even manual partitioning techniques. These static algorithms are a good starting point for our work and would likely yield better results for key/value stores given their simplified data access model. Also, an incremental version of these algorithms may scale better than the static equivalent, since it may be able to consider only new transaction log entries in each iteration. Another option would be to use a probabilistic algorithm.



\section{Methods}
We will use an empirical approach to designing and evaluating our proposal. We plan to design an implement our system as an extension to existing OLTP Cloud Data Stores. In particular, we plan to demonstrate its applicability to both partitioning based and non-partitioning based approaches. Therefore, we will select a system from each category and extend it with dynamic partitioning.

We plan to focus on key/value stores, specifically solutions that build on HBase, the open source implementation of BigTable. For the partition category we plan to use the Megastore design. Since this is a proprietary, closed source product, we will attempt to rebuild the important parts of this system on top of HBase as a basis for evaluation. For the non-partitioning based category we have chosen CloudTPS. Since source code of the research prototype of this system is publicly available, we plan to directly extend it.

The components of the system consist of: (1) a data store agnostic, incremental partition detection algorithm, (2) data store-specific transaction log observers, (3) data store-specific runtime partitioners.

We will evaluate our design by comparing the number of transactions per minute of no grouping, vs static groups, vs automatic groups. We also plan to consider different access patterns.

\section{Results}
We expect the automatic partitioning to introduce some overhead into the system. However, we hope to design the algorithm such that the overhead is minimal for relatively static access patterns so that the cost is amortized. We will also attempt to keep the overhead of partitioning the data small.

\section{Discussion}


% Bibliography generation from references.bib
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
